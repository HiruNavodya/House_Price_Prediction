{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977e66c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from ydata_profiling import ProfileReport\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from itertools import combinations\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from collections import Counter\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac4197",
   "metadata": {},
   "source": [
    "# Without PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb7a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data (3).csv', index_col='date')\n",
    "def Basic_Preprocessing(df):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.index = df.index.date  \n",
    "\n",
    "    \n",
    "    df.drop(columns=['country','street'],inplace=True)\n",
    "    df = df[df['price'] != 0]\n",
    "    df['yr_renovated'] = df['yr_renovated'].apply(lambda x: 1 if x!=0 else 0)\n",
    "    df['waterfront'] =  df['waterfront'].apply(lambda x: 1 if x!=0 else 0)\n",
    "    df['yr_built'] = df['yr_built'].apply(lambda x: 1 if x > df['yr_built'].median() else 0)\n",
    "    df['sqft_total'] = df['sqft_basement'] + df['sqft_above']\n",
    "    df.drop(columns=['sqft_basement','sqft_above'],inplace=True)\n",
    "    #df.drop(columns=['statezip'],inplace=True)\n",
    "    df['statezip'] = df['statezip'].apply(lambda x: int(str(x).split()[1]))\n",
    "    df['month'] = pd.to_datetime(df.index).month\n",
    "    df['date'] = pd.to_datetime(df.index).day\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df \n",
    "\n",
    "df = Basic_Preprocessing(data)\n",
    "\n",
    "X = df.drop(columns=['price'])\n",
    "Y= df['price']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11aa6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1551d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###########  One hot encoding \n",
    "combined_df = pd.concat([X_train, X_test])\n",
    "dummy_cols = ['city']\n",
    "combined_dummy_df = pd.get_dummies(combined_df[dummy_cols], drop_first=True)\n",
    "X_train_dummy = combined_dummy_df.iloc[:len(X_train)]\n",
    "X_test_dummy = combined_dummy_df.iloc[len(X_train):]\n",
    "X_train_dummy = X_train_dummy.astype(int)\n",
    "X_test_dummy = X_test_dummy.astype(int)\n",
    "X_train = X_train.drop(columns=dummy_cols)\n",
    "X_test = X_test.drop(columns=dummy_cols)\n",
    "X_train = pd.concat([X_train, X_train_dummy], axis=1)\n",
    "X_test = pd.concat([X_test, X_test_dummy], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b14d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest(contamination=0.1)  \n",
    "clf.fit(X_train)\n",
    "y_pred = clf.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a618d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['cluster_labels'] = y_pred\n",
    "X_train_filtered = X_train[X_train['cluster_labels'] == 1]\n",
    "indexes_1 = X_train_filtered.index\n",
    "y_train_filtered = y_train.loc[indexes_1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d66531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_transform = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "       'view', 'condition', 'sqft_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba417f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform on training data\n",
    "X_train_filtered =X_train_filtered.copy()  # Create a copy to avoid modifying the original data\n",
    "X_train_filtered[vars_to_transform] = scaler.fit_transform(X_train_filtered[vars_to_transform])\n",
    "\n",
    "# Transform test data using the scaler fitted on training data\n",
    "X_test =X_test.copy()  # Create a copy to avoid modifying the original data\n",
    "X_test[vars_to_transform] = scaler.transform(X_test[vars_to_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c16fafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_filtered = np.log1p(y_train_filtered)\n",
    "y_test = np.log1p(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240f65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ca4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01f24709",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_params = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.5, 0.75, 1.0],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create GradientBoostingRegressor\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=gb_reg, param_grid=gb_params,\n",
    "                           cv=5, scoring='neg_mean_squared_error',\n",
    "                           n_jobs=-1)\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# Get the best model\n",
    "best_gb_model_1 = grid_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c56dfb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-squared: 0.8867311082309014\n",
      "Adjusted R-squared (Test): 0.9598394873994065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "best_gb_model_1.fit(X_train_filtered, y_train_filtered)\n",
    "y_train_pred = best_gb_model_1.predict(X_train_filtered)\n",
    "r_squared = r2_score(y_train_filtered, y_train_pred)\n",
    "n = len(y_train)\n",
    "p =X_train_filtered.shape[1]\n",
    "adjusted_r_squared = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "print(\"Adjusted R-squared:\", adjusted_r_squared)\n",
    "\n",
    "########## calculate the adj R squred value for testing ##########\n",
    "best_gb_model_1.fit(X_test, y_test)\n",
    "y_test_pred = best_gb_model_1.predict(X_test)\n",
    "r_squared_test = r2_score(y_test, y_test_pred)\n",
    "n_test = len(y_test)\n",
    "p_test = X_test.shape[1]\n",
    "adjusted_r_squared_test = 1 - (1 - r_squared_test) * (n_test - 1) / (n_test - p_test - 1)\n",
    "print(\"Adjusted R-squared (Test):\", adjusted_r_squared_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1533178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (Training): 0.166732687789927\n",
      "RMSE (Testing): 0.10501003588758821\n"
     ]
    }
   ],
   "source": [
    "rmse_train = np.sqrt(mean_squared_error(y_train_filtered, y_train_pred))\n",
    "print(\"RMSE (Training):\", rmse_train)\n",
    "# Calculate RMSE for testing set\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(\"RMSE (Testing):\", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4ae6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e07efbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "sqft_total: 0.20356602013367198\n",
      "sqft_living: 0.13108069516698892\n",
      "bathrooms: 0.08325868433925443\n",
      "statezip: 0.06951950729654968\n",
      "sqft_lot: 0.05914294821623591\n",
      "bedrooms: 0.05663561155192297\n",
      "view: 0.043805743353218435\n",
      "city_Kent: 0.0283432338593558\n",
      "city_Bellevue: 0.027838473451168674\n",
      "city_Auburn: 0.024049301637542275\n",
      "city_Mercer Island: 0.023657233727055058\n",
      "date: 0.02184247963455702\n",
      "city_Seattle: 0.020940424481868093\n",
      "city_Federal Way: 0.01823869475642739\n",
      "city_Tukwila: 0.017496622813805752\n",
      "city_Renton: 0.01727473402355781\n",
      "city_Covington: 0.01577379127951426\n",
      "city_Yarrow Point: 0.015569705399316236\n",
      "floors: 0.014360791426782147\n",
      "city_Redmond: 0.013936092750795452\n",
      "city_Des Moines: 0.013925013800256522\n",
      "waterfront: 0.011784449763335675\n",
      "city_Clyde Hill: 0.009322311292936836\n",
      "condition: 0.007694626707719203\n",
      "city_Maple Valley: 0.007650271507744456\n",
      "yr_built: 0.007150260775733409\n",
      "city_Kirkland: 0.004992967611725976\n",
      "month: 0.004423990154238015\n",
      "yr_renovated: 0.003763825754636304\n",
      "city_Sammamish: 0.0032350769586964646\n",
      "city_North Bend: 0.0025166590240287955\n",
      "city_Beaux Arts Village: 0.002489250109007835\n",
      "city_Burien: 0.0022003158860117314\n",
      "city_Issaquah: 0.0020361602467425734\n",
      "city_Shoreline: 0.0016180885487609683\n",
      "city_Kenmore: 0.0014846303808544084\n",
      "city_Lake Forest Park: 0.001186362445120483\n",
      "city_SeaTac: 0.0010584602181557\n",
      "city_Snoqualmie: 0.0009020915031464324\n",
      "city_Normandy Park: 0.0007378973546440584\n",
      "city_Pacific: 0.0006772581063319136\n",
      "city_Newcastle: 0.0005682019488559609\n",
      "city_Duvall: 0.0004893658865560988\n",
      "city_Bothell: 0.00048203475174658826\n",
      "city_Vashon: 0.00034364661895215767\n",
      "city_Enumclaw: 0.0003261639243114597\n",
      "city_Woodinville: 0.00024985241131820433\n",
      "city_Fall City: 0.0001508942304573564\n",
      "city_Inglewood-Finn Hill: 6.469512266488102e-05\n",
      "city_Ravensdale: 5.6269041833191966e-05\n",
      "city_Carnation: 5.4704512095946706e-05\n",
      "city_Preston: 3.3414101792130134e-05\n",
      "city_Black Diamond: 0.0\n",
      "city_Medina: 0.0\n",
      "city_Milton: 0.0\n",
      "city_Skykomish: 0.0\n",
      "city_Snoqualmie Pass: 0.0\n"
     ]
    }
   ],
   "source": [
    "feature_importances = best_gb_model_1.feature_importances_\n",
    "feature_names = X_train_filtered.columns\n",
    "feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88c1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e831e8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a02134f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594ab51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8213f028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca9fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda5f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAPE for training set\n",
    "mape_train = np.mean(np.abs((y_train_filtered - y_train_pred) / y_train_filtered)) * 100\n",
    "print(\"Mean Absolute Percentage Error (Training):\", mape_train)\n",
    "\n",
    "# Calculate MAPE for testing set\n",
    "mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "print(\"Mean Absolute Percentage Error (Testing):\", mape_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314d4014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf9327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = best_gb_model_1.feature_importances_\n",
    "feature_names = X_train_filtered.columns\n",
    "feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
    "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5c614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33610706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c3291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679729b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b37213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5c15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a62c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-squared (Random Forest - Training): 0.6918285662792178\n",
      "Adjusted R-squared (Random Forest - Testing): 0.7497954129057374\n",
      "RMSE (Random Forest - Training): 0.27498718237786884\n",
      "RMSE (Random Forest - Testing): 0.26210711907325335\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor()\n",
    "\n",
    "# Grid search\n",
    "rf_grid_search = GridSearchCV(estimator=rf_reg, param_grid=rf_params,\n",
    "                              cv=5, scoring='neg_mean_squared_error',\n",
    "                              n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "rf_grid_search.fit(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# Get the best Random Forest model\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "##########################################################################################################\n",
    "###########################################################################################################\n",
    "##########   calculate  the adj R squred value   for training \n",
    "from sklearn.metrics import r2_score\n",
    "best_rf_model.fit(X_train_filtered, y_train_filtered)\n",
    "y_train_pred_rf = best_rf_model.predict(X_train_filtered)\n",
    "r_squared_rf = r2_score(y_train_filtered, y_train_pred_rf)\n",
    "n_train = len(y_train_filtered)\n",
    "p_train = X_train_filtered.shape[1]\n",
    "adjusted_r_squared_rf = 1 - (1 - r_squared_rf) * (n_train - 1) / (n_train - p_train - 1)\n",
    "print(\"Adjusted R-squared (Random Forest - Training):\", adjusted_r_squared_rf)\n",
    "\n",
    "##########   calculate  the adj R squred value   for testing ##########y_test_pred = best_gb_model_1.predict(x_test_cluster_1)\n",
    "best_rf_model.fit(X_test, y_test)\n",
    "y_test_pred_rf = best_rf_model.predict(X_test)\n",
    "r_squared_test_rf = r2_score(y_test, y_test_pred_rf)\n",
    "n_test = len(y_test)\n",
    "p_test = X_test.shape[1]\n",
    "adjusted_r_squared_test_rf = 1 - (1 - r_squared_test_rf) * (n_test - 1) / (n_test - p_test - 1)\n",
    "print(\"Adjusted R-squared (Random Forest - Testing):\", adjusted_r_squared_test_rf)\n",
    "\n",
    "######################################################################################\n",
    "# Calculate RMSE for training set\n",
    "rmse_train_rf = np.sqrt(mean_squared_error(y_train_filtered, y_train_pred_rf))\n",
    "print(\"RMSE (Random Forest - Training):\", rmse_train_rf)\n",
    "\n",
    "# Calculate RMSE for testing set\n",
    "rmse_test_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
    "print(\"RMSE (Random Forest - Testing):\", rmse_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9879dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAPE for training set\n",
    "mape_train = np.mean(np.abs((y_train_filtered - y_train_pred) / y_train_filtered)) * 100\n",
    "print(\"Mean Absolute Percentage Error (Training):\", mape_train)\n",
    "\n",
    "# Calculate MAPE for testing set\n",
    "mape_test = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "print(\"Mean Absolute Percentage Error (Testing):\", mape_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6502b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 300}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e448ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Percentage Error (Random Forest - Training): 1.5761137518777706\n",
      "Mean Absolute Percentage Error (Random Forest - Testing): 1.5691654296427504\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAPE for training set\n",
    "mape_train_rf = np.mean(np.abs((y_train_filtered - y_train_pred_rf) / y_train_filtered)) * 100\n",
    "print(\"Mean Absolute Percentage Error (Random Forest - Training):\", mape_train_rf)\n",
    "\n",
    "# Calculate MAPE for testing set\n",
    "mape_test_rf = np.mean(np.abs((y_test - y_test_pred_rf) / y_test)) * 100\n",
    "print(\"Mean Absolute Percentage Error (Random Forest - Testing):\", mape_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5dd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b609e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70bb6da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5ed26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ef656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e787a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1716f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cebff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138dc1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620059ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d717e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d9aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32444c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c987014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d9248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83045661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c043bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
